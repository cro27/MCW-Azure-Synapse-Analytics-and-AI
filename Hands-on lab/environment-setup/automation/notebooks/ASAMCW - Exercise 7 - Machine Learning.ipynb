{
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "sessionKeepAliveTimeout": 30
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train a classifier to determine product seasonality\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import all necessary libraries.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.externals import joblib\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "from onnxmltools.convert import convert_xgboost\n",
        "from onnxmltools.convert.common.data_types import FloatTensorType\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "\n",
        "from azureml.core.experiment import Experiment\n",
        "from azureml.core.workspace import Workspace\n",
        "from azureml.train.automl.run import AutoMLRun\n",
        "from azureml.train.automl import AutoMLConfig\n",
        "from azureml.automl.runtime.onnx_convert import OnnxConverter\n",
        "from azureml.core.model import Model\n",
        "from azureml.core import Environment\n",
        "from azureml.core.model import InferenceConfig\n",
        "from azureml.core.webservice import AciWebservice\n",
        "from azureml.core.webservice import Webservice"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exploratory data analysis (basic stats)\n",
        "\n",
        "Create Spark temporary views for sales and products."
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "diagram": {
          "activateDiagramType": 2,
          "chartConfig": {
            "category": "bar",
            "keys": [
              "Seasonality"
            ],
            "values": [
              "ProductId"
            ],
            "yLabel": "ProductId",
            "xLabel": "Seasonality",
            "aggregation": "SUM",
            "aggByBackend": false
          },
          "aggData": {
            "ProductId": {
              "1": 913878,
              "2": 273395,
              "3": 267388
            }
          },
          "isSummary": false,
          "previewData": {
            "filter": null
          },
          "isSql": false
        }
      },
      "source": [
        "%%spark\n",
        "val df = spark.read.sqlanalytics(\"SQLPool01.wwi_mcw.SaleSmall\") \n",
        "df.createOrReplaceTempView(\"sale\")\n",
        "\n",
        "val df2 = spark.read.sqlanalytics(\"SQLPool01.wwi_mcw.Product\") \n",
        "df2.createOrReplaceTempView(\"product\")\n",
        "display(df2)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load daily product sales from the SQL pool.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "sqlQuery = \"\"\"\n",
        "SELECT\n",
        "    P.ProductId\n",
        "    ,P.Seasonality\n",
        "    ,S.TransactionDateId\n",
        "    ,COUNT(*) as TransactionItemsCount\n",
        "FROM\n",
        "    sale S\n",
        "    JOIN product P ON\n",
        "        S.ProductId = P.ProductId\n",
        "WHERE TransactionDateId between 20190101 and 20191231\n",
        "GROUP BY\n",
        "    P.ProductId\n",
        "    ,P.Seasonality\n",
        "    ,S.TransactionDateId\n",
        "\"\"\"\n",
        "\n",
        "prod_df = spark.sql(sqlQuery)\n",
        "prod_df.cache()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check the number of records in the data frame (should be around 1.3 million rows)."
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "prod_df.count()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Display some statistics about the data frame.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "outputCollapsed": true,
        "diagram": {
          "activateDiagramType": 1,
          "chartConfig": {
            "category": "bar",
            "keys": [
              "summary"
            ],
            "values": [
              "summary"
            ],
            "yLabel": "summary",
            "xLabel": "summary",
            "aggregation": "COUNT",
            "aggByBackend": false
          },
          "aggData": {
            "summary": {
              "count": 1,
              "max": 1,
              "mean": 1,
              "min": 1,
              "stddev": 1
            }
          },
          "isSummary": false,
          "previewData": {
            "filter": null
          },
          "isSql": false
        }
      },
      "source": [
        "display(prod_df.describe())"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pivot the data frame to make daily sale items counts columns. \n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "prod_prep_df = prod_df.groupBy(['ProductId', 'Seasonality']).pivot('TransactionDateId').sum('TransactionItemsCount').toPandas()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Clean up the nulls and take a look at the result.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "prod_prep_df = prod_prep_df.fillna(0)\n",
        "prod_prep_df.head(10)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Isolate features and prediction classes.\n",
        "\n",
        "Standardize features by removing the mean and scaling to unit variance.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "X = prod_prep_df.iloc[:, 2:].values\n",
        "y = prod_prep_df['Seasonality'].values\n",
        "\n",
        "X_scale = StandardScaler().fit_transform(X)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Use PCA for dimensionality reduction\n",
        "\n",
        "Perform dimensionality reduction using Principal Components Analysis and two target components.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "pca = PCA(n_components=2)\n",
        "principal_components = pca.fit_transform(X_scale)\n",
        "principal_components = MinMaxScaler().fit_transform(principal_components)\n",
        "\n",
        "pca_df = pd.DataFrame(data = principal_components, columns = ['pc1', 'pc2'])\n",
        "pca_df = pd.concat([pca_df, prod_prep_df[['Seasonality']]], axis = 1)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Display the products data frame in two dimensions (mapped to the two principal components).\n",
        "\n",
        "Note the clear separation of clusters.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "fig = plt.figure(figsize = (6,6))\n",
        "ax = fig.add_subplot(1,1,1) \n",
        "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
        "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
        "ax.set_title('2 component PCA', fontsize = 20)\n",
        "targets = [1, 2, 3]\n",
        "colors = ['r', 'g', 'b']\n",
        "for target, color in zip(targets,colors):\n",
        "    indicesToKeep = pca_df['Seasonality'] == target\n",
        "    ax.scatter(pca_df.loc[indicesToKeep, 'pc1']\n",
        "               , pca_df.loc[indicesToKeep, 'pc2']\n",
        "               , c = color\n",
        "               , s = 1)\n",
        "ax.legend(['All Season Products', 'Summer Products', 'Winter Products'])\n",
        "ax.plot([-0.05, 1.05], [0.77, 1.0], linestyle=':', linewidth=1, color='y')\n",
        "ax.plot([-0.05, 1.05], [0.37, 0.6], linestyle=':', linewidth=1, color='y')\n",
        "ax.grid()\n",
        "\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Redo the Principal Components Analysis, this time with twenty dimensions.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "def col_name(x):\n",
        "    return f'f{x:02}'\n",
        "\n",
        "pca = PCA(n_components=20)\n",
        "principal_components = pca.fit_transform(X_scale)\n",
        "principal_components = MinMaxScaler().fit_transform(principal_components)\n",
        "\n",
        "X = pd.DataFrame(data = principal_components, columns = list(map(col_name, np.arange(0, 20))))\n",
        "pca_df = pd.concat([X, prod_prep_df[['ProductId']]], axis = 1)\n",
        "pca_automl_df = pd.concat([X, prod_prep_df[['Seasonality']]], axis = 1)\n",
        "\n",
        "X = X[:4500]\n",
        "y = prod_prep_df['Seasonality'][:4500]\n",
        "pca_automl_df = pca_automl_df[:4500]"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save the PCA components to the SQL pool (you may ignore any warnings).\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "pca_sdf = spark.createDataFrame(pca_df)\n",
        "pca_sdf.createOrReplaceTempView(\"productpca\")"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "outputCollapsed": true
      },
      "source": [
        "%%spark\n",
        "val df = spark.sqlContext.sql(\"select * from productpca\")\n",
        "df.write.sqlanalytics(\"SQLPool01.wwi_mcw.ProductPCA\", Constants.INTERNAL)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train ensemble of trees classifier (using XGBoost)\n",
        "\n",
        "Split into test and training data sets.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=123)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train the ensemble classifier using XGBoost.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "model = XGBClassifier()\n",
        "model.fit(X_train, y_train)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Perform predictions with the newly trained model.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "y_pred = model.predict(X_test)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculate the accuracy of the model using test data.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train classifier using Auto ML\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Configure the connection to the Azure Machine Learning workspace. \r\n",
        "\r\n",
        "Please add your subscriptionID, Resource Group name and Azure machine Learning Workspace name which can be found in the Azure Portal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "subscription_id= '#SUBSCRIPTION_ID#'\n",
        "resource_group= '#RESOURCE_GROUP_NAME#'\n",
        "workspace_name= '#AML_WORKSPACE_NAME#'\n",
        "ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n",
        "ws.write_config()\n",
        "\n",
        "experiment = Experiment(ws, \"ASAMCW_Product_Seasonality\")"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Configure the Automated Machine Learning experiment and start it (will run on local compute resources).\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "automl_classifier_config = AutoMLConfig(\n",
        "        task='classification',        \n",
        "        experiment_timeout_minutes=15,\n",
        "        enable_onnx_compatible_models=True,\n",
        "        training_data=pca_automl_df,\n",
        "        label_column_name='Seasonality',\n",
        "        n_cross_validations=5,\n",
        "        enable_voting_ensemble=False,\n",
        "        enable_stack_ensemble=False\n",
        "        )\n",
        "\n",
        "local_run = experiment.submit(automl_classifier_config, show_output=True)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Operationalize\n",
        "Operationalization means getting the model into the cloud so that others can run it after you close the notebook. \n",
        "\n",
        "While you can create a docker container running on Azure Container Instances (ACI) to host the model, we want to host the model in Synapse Analytics so it can be called from T-SQL Predict function to easily enrich your SQL Pool data.\n",
        "PREDICT() requires the model to be in ONNX format so we need to retrieve the best ONNX model from the AutoML pipeline iterations. The model is then registered with Azure Machine Learning model registery. \n",
        "\n",
        "We will later use the ONNX model for inferencing in Azure Synapse SQL Pool using the new model scoring wizard."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Retrieve the best ONNX odel\n",
        "\n",
        "The get_output method returns the best run (best_run) and the fitted model (onnx_mdl). The Model includes the pipeline and any pre-processing.\n"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "best_run, onnx_mdl = local_run.get_output(return_onnx_model=True)\n"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save the best ONNX model\n"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "from azureml.automl.runtime.onnx_convert import OnnxConverter\n",
        "onnx_fl_path = \"./best_model.onnx\"\n",
        "OnnxConverter.save_onnx_model(onnx_mdl, onnx_fl_path)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Register model with Azure Machine Learning model registery using MLflow\n"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "import mlflow\n",
        "import mlflow.onnx\n",
        "import pandas\n",
        "\r\n",
        "from mlflow.models.signature import infer_signature\n",
        "\r\n",
        "experiment_name = 'ASAMCW_Product_Seasonality'\n",
        "artifact_path = 'ASAMCW_Product_Seasonality_artifact'\n",
        "\r\n",
        "mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\n",
        "mlflow.set_experiment(experiment_name)\n",
        "\n",
        "with mlflow.start_run() as run:\n",
        "    # Infer signature\n",
        "    input_sample = X_train.head(1)\n",
        "    output_sample = pandas.DataFrame(columns=['output_label'], data=[1])\n",
        "    signature = infer_signature(input_sample, output_sample)\n",
        "\r\n",
        "    # Save the model to the outputs directory for capture\r\n",
        "    mlflow.onnx.log_model(onnx_mdl, artifact_path, signature=signature, input_example=input_sample)\n",
        "\n",
        "    # Register the model to AML model registry\n",
        "    mlflow.register_model('runs:/' + run.info.run_id + '/' + artifact_path, 'ASAMCW_Product_Seasonality')"
      ],
      "attachments": {}
    },
    "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can test the model predictions here in the notebook using onnxruntime package"
       ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "import sys\n",
        "import json\n",
        "from azureml.automl.core.onnx_convert import OnnxConvertConstants\n",
        "from azureml.train.automl import constants\n",
        "\r\n",
        "if sys.version_info < OnnxConvertConstants.OnnxIncompatiblePythonVersion:\n",
        "    python_version_compatible = True\n",
        "else:\r\n",
        "    python_version_compatible = False\n",
        "\r\n",
        "import onnxruntime\r\n",
        "from azureml.automl.runtime.onnx_convert import OnnxInferenceHelper\n",
        "\n",
        "def get_onnx_res(run):\n",
        "    res_path = 'onnx_resource.json'\n",
        "    run.download_file(name=constants.MODEL_RESOURCE_PATH_ONNX, output_file_path=res_path)\n",
        "    with open(res_path) as f:\n",
        "        onnx_res = json.load(f)\n",
        "    return onnx_res\n",
        "\r\n",
        "if python_version_compatible:\n",
        "    test_df = X_test\n",
        "    mdl_bytes = onnx_mdl.SerializeToString()\n",
        "    onnx_res = get_onnx_res(best_run)\n",
        "\r\n",
        "    onnxrt_helper = OnnxInferenceHelper(mdl_bytes, onnx_res)\n",
        "    pred_onnx, pred_prob_onnx = onnxrt_helper.predict(test_df)\n",
        "\n",
        "    print(pred_onnx)\n",
        "    print(pred_prob_onnx)\n",
        "else:\r\n",
        "    print('Please use Python version 3.6 or 3.7 to run the inference helper.')\n"
      ],
      "attachments": {}
    }
  ]
}
